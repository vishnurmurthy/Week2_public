{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sleep_Analysis_Challenge.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "FB6YunD0Gdgc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# \"You Snooze, You Win\" Challenge\n",
        "\n",
        "Every year, the [PhysioNet/CinC (Computing in Cardiology) Challenge](https://www.physionet.org/challenge/) invites \"participants to tackle clinically interesting problems that are either unsolved or not well-solved.\" As you may have figured out from the title, this year's challenge focuses on sleep, particularly the classification of nonarousal and arousal timeframes. If you would like to understand the biological implications of the challenge, we recommend reading PhysioNet's [introduction](https://physionet.org/challenge/2018/) of the challenge.\n",
        "\n",
        "For this challenge, you will classify samples into 5 classes (Arousal, NREM1, NREM2, NREM3, REM). Each sample consists of seven physiological signals (O2-M1, E1-M2, Chin1-Chin2, ABD, CHEST, AIRFLOW, ECG) measured at 200 Hz over a 60 second period (12000 timepoints). In this notebook, we provide code to import the data, visualize sample signals, implement an example classifier, and 'score' your model."
      ]
    },
    {
      "metadata": {
        "id": "7ctSzKHoGr1q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "cc515b65-b10d-4b13-d701-9d433dfcaf78"
      },
      "cell_type": "code",
      "source": [
        "### Run once ###\n",
        "! pip install peakutils\n",
        "from scipy.interpolate import *\n",
        "import peakutils\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import sys\n",
        "import random\n",
        "from sklearn import metrics\n",
        "from sklearn.utils import shuffle\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from scipy.signal import *\n",
        "! pip install neurokit\n",
        "import neurokit as nk\n",
        "tf.enable_eager_execution()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: peakutils in /usr/local/lib/python3.6/dist-packages (1.1.1)\r\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from peakutils) (1.14.5)\r\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from peakutils) (0.19.1)\n",
            "Requirement already satisfied: neurokit in /usr/local/lib/python3.6/dist-packages (0.2.0)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from neurokit) (0.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from neurokit) (4.0.0)\n",
            "Requirement already satisfied: mne in /usr/local/lib/python3.6/dist-packages (from neurokit) (0.16.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from neurokit) (2.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from neurokit) (0.19.1)\n",
            "Requirement already satisfied: biosppy in /usr/local/lib/python3.6/dist-packages (from neurokit) (0.5.1)\n",
            "Requirement already satisfied: cvxopt in /usr/local/lib/python3.6/dist-packages (from neurokit) (1.2.0)\n",
            "Requirement already satisfied: nolds in /usr/local/lib/python3.6/dist-packages (from neurokit) (0.4.1)\n",
            "Requirement already satisfied: bioread in /usr/local/lib/python3.6/dist-packages (from neurokit) (1.0.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from neurokit) (0.22.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from neurokit) (1.14.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->neurokit) (0.19.2)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->neurokit) (0.45.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->neurokit) (2.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->neurokit) (2.5.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->neurokit) (0.10.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->neurokit) (1.11.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from matplotlib->neurokit) (2018.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from nolds->neurokit) (39.1.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from nolds->neurokit) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_XxIE6ZwGvjh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loading the Dataset\n",
        "\n",
        "This dataset is a modified version of the PhysioNet/CinC Challenge data, which were contributed by the Massachusetts General Hospital’s Computational Clinical Neurophysiology Laboratory, and the Clinical Data Animation Laboratory.\n",
        "***\n",
        "**Class labels:**\n",
        "- 0 = Arousal\n",
        "- 1 = NREM1\n",
        "- 2 = NREM2\n",
        "- 3 = NREM3\n",
        "- 4 = REM\n",
        "***\n",
        "**Class description:**\n",
        "\n",
        "|Class|State|Characterizations|Brain Waves|Percentage of Sleep|\n",
        "|-|-|-|-|-|-|\n",
        "|Arousal|Consciousness|Wakefulness (coherent cognitive and behavioral responses to external stimuli)|Alpha, Beta|-|\n",
        "|NREM1|Light sleep|Hypnic jerk (involuntary twitch that causes an individual to awaken for a moment)|Theta|5|\n",
        "|NREM2|Unequivocal sleep|Sleep spindle (sudden burst of oscillatory brain activity); K-complex (delta wave that lasts for a second)|Theta, Delta|40-50|\n",
        "|NREM3|Deep sleep|Parasomnias (sleep disorders such as sleepwalking and night terrors)|Delta|15-25|\n",
        "|REM|Dreaming sleep|REM atonia (paralysis of nonessential skeletal muscles); Dreaming|Alpha, Beta|20-25|\n",
        "***\n",
        "**Physiological signal description:**\n",
        "\n",
        "O2-M1 - posterior brain activity (electroencephalography)\n",
        "\n",
        "E1-M2 - left eye activity (electrooculography)\n",
        "\n",
        "Chin1-Chin2 - chin movement (electromyography)\n",
        "\n",
        "ABD - abdominal movement (electromyography)\n",
        "\n",
        "CHEST - chest movement (electromyography)\n",
        "\n",
        "AIRFLOW - respiratory airflow\n",
        "\n",
        "ECG - cardiac activity (electrocardiography)\n",
        "***\n",
        "Run both cell blocks to get the challenge data."
      ]
    },
    {
      "metadata": {
        "id": "yfgbZPNYziXt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "da02e48d-7d29-4767-9ba9-ff42f9620095"
      },
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/BeaverWorksMedlytics/datasets.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'datasets' already exists and is not an empty directory.\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "k_iKzgvyzioe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.chdir('./datasets/Week2_Challenge/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qANF2BvQG2m3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Visualization\n",
        "\n",
        "Run the first cell once to store the training and test sample file locations. If the data have been imported correctly, the cell should output '4000' (training) and '1000' (test).\n",
        "\n",
        "Run the second cell once to initialize important functions that you may find useful. Descriptions of input and output will be provided for every function in the notebook.\n",
        "\n",
        "Run the last cell to visualize a random test sample's seven physiological signals in a raw and FFT (fast fourier transform) format. Note that you can change different parameters, which we will go over in detail.\n",
        "\n",
        "It is important to recognize that the same signal from different samples in the same class may vary in terms of amplitude and frequency. Of course, this is a byproduct of intraspecies variation. Further data preprocessing and/or discriminatory feature extraction may be necessary to account for this phenomenon."
      ]
    },
    {
      "metadata": {
        "id": "-Rw8inOvG5QP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "75b2c205-2252-4946-832c-023b8610e3f6"
      },
      "cell_type": "code",
      "source": [
        "### Run once ###\n",
        "\n",
        "def get_file_locs():\n",
        "    file_dict = {'training':[], 'test':[]}\n",
        "    for data_type in file_dict:\n",
        "        for file in os.listdir('./' + data_type):\n",
        "            file_dict[data_type].append(data_type + '/' + file)\n",
        "    \"\"\" file_dict (dict) stores 'training' and 'test' as keys and sample file locations as values \"\"\"\n",
        "    return file_dict\n",
        "\n",
        "file_dict = get_file_locs()\n",
        "print(len(file_dict['training']), len(file_dict['test']))\n",
        "\n",
        "file_dict = get_file_locs()\n",
        "print(len(file_dict['training']), len(file_dict['test']))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4000 1000\n",
            "4000 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "G6x9vTz5HE1R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_sample_data(data_type, id_number):\n",
        "    file = './' + data_type + '/' + str(id_number) + '.xz'\n",
        "    sample = pd.read_pickle('./' + file)\n",
        "    return sample, file.split('/')[2]\n",
        "\n",
        "def get_raw_signals(sample):\n",
        "    raw_signals_x = np.arange(0, 60, step = 1/200)\n",
        "    raw_signals_y = np.transpose(sample['Signal'][0])\n",
        "    \"\"\" tuple: raw_signals_x (ndarray) stores raw signal timepoints in seconds; raw_signals_y (ndarray) stores all seven signals for every timepoint; 'Raw' (str) stores signal type \"\"\"\n",
        "    return (raw_signals_x, raw_signals_y, 'Raw')\n",
        "\n",
        "def get_data(data_type, num_samples): # transposed into 4000, 7 \n",
        "    raw, order = np.array([]), np.array([])\n",
        "    for i in range(num_samples):\n",
        "        sample, sample_id = get_sample_data(data_type, i)\n",
        "        print(\"dt\",data_type, \"i\",i, \"sample_id\",sample_id)\n",
        "        print(\"sample:\", sample)\n",
        "        print()\n",
        "        sample_raw = get_raw_signals(sample)\n",
        "        np.append(raw, sample_raw)\n",
        "        print(sample_raw)\n",
        "        print(raw)\n",
        "        sys.exit(0)\n",
        "    return raw, order\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zjgpE5L3Chco",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "3580192f-1c9b-4f47-fad8-c8512b6db36a"
      },
      "cell_type": "code",
      "source": [
        "train_data = get_data('training', 4000)\n",
        "test_data = get_data('test', 4000)\n",
        "\n",
        "### Run once ###\n",
        "\n",
        "train_labels = np.ndarray(shape = (1, 4000))\n",
        "for i in range(4000):\n",
        "    train_labels[0][i] = i//800\n",
        "train_labels = tf.keras.utils.to_categorical(train_labels[0], 5)\n",
        "train_labels = train_labels.astype(np.float32)\n",
        "\n",
        "print(train_labels)\n",
        "\n",
        "### Run every time you modify your feature extraction ###\n",
        "train_data = train_data.astype(np.float32)\n",
        "test_data = test_data.astype(np.float32)\n",
        "train_labels = train_labels.astype(np.float32)\n",
        "\n",
        "print(train_data.shape, train_labels.shape, '\\n\\n', train_data, '\\n\\n', train_labels)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dt training i 0 sample_id 0.xz\n",
            "sample:                                               Signal  Label\n",
            "0  [[3.0, -9.0, -4.0, 71.0, -9.0, 19.0, 1.0], [0....      0\n",
            "\n",
            "(array([0.0000e+00, 5.0000e-03, 1.0000e-02, ..., 5.9985e+01, 5.9990e+01,\n",
            "       5.9995e+01]), array([[   3.,    0.,    1., ...,   -6.,   -1.,    4.],\n",
            "       [  -9.,  -11.,  -11., ...,    5.,    4.,   -4.],\n",
            "       [  -4.,   -7.,    4., ...,   24.,   10.,    3.],\n",
            "       ...,\n",
            "       [  -9.,   -9.,  -10., ..., -113., -113., -113.],\n",
            "       [  19.,   20.,   19., ...,   25.,   26.,   26.],\n",
            "       [   1.,  -10.,   -6., ...,  -10.,   -9.,  -20.]]), 'Raw')\n",
            "[]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ERFUkgep2N5s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### Run every time you modify your feature extraction ###\n",
        "\n",
        "train_data, train_labels = shuffle(train_data, train_labels)\n",
        "\n",
        "\"\"\" val_size (int) must be from 0-4000 \"\"\"\n",
        "val_size = 1000\n",
        "mocktest_size = 500\n",
        "\n",
        "val_data = train_data[:val_size]\n",
        "mocktest_data = train_data[val_size:val_size + mocktest_size]\n",
        "partial_train_data = train_data[val_size + mocktest_size:]\n",
        "\n",
        "val_labels = train_labels[:val_size]\n",
        "mocktest_labels = train_labels[val_size:val_size + mocktest_size]\n",
        "partial_train_labels = train_labels[val_size + mocktest_size:]\n",
        "\n",
        "training_set = tf.data.Dataset.from_tensor_slices((partial_train_data, partial_train_labels))\n",
        "training_set = training_set.shuffle(partial_train_labels.shape[0]).batch(40)\n",
        "\n",
        "twod_array = np.ndarray(shape=(4000, 84000))\n",
        "for i in range(4000):\n",
        "  sample, sample_id = get_sample_data('training', i)\n",
        "  sample_raw = get_raw_signals(sample)\n",
        "  pancake = sample_raw[1].flatten()\n",
        "  twod_array[i] = pancake\n",
        "  \n",
        "print(twod_array)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mXsZa50WHGZR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Example Classifier\n",
        "\n",
        "Below is an example of a poor classifier for this dataset. It is a simple neural network that takes the maximal, normalized variance for each signal (given a frame and interval) as its input features. After training, it performs with 44% accuracy on test data.\n",
        "\n",
        "While the example classifier makes use of a neural network, we encourage you to utilize any ML algorithm that you feel would be appropriate. Also, note that this example classifier makes use of only the raw signals, without consideration of the FFT signals (or any other processed form of the raw signals)."
      ]
    },
    {
      "metadata": {
        "id": "RUhVo7jkHIdS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### Run every time you change your model parameters ###\n",
        "\n",
        "#model = tf.keras.Sequential()\n",
        "\n",
        "\"\"\" Modify to your heart's and algorithm's content ^_^ \"\"\"\n",
        "\n",
        "#model.add(keras.layers.Conv1D(64, kernel_size=3,\n",
        "#                 activation=tf.nn.relu,\n",
        "#                 input_shape=(4000,7)))\n",
        "\n",
        "#model.add(tf.keras.layers.Dense(64, activation=tf.nn.relu, input_shape=(train_data.shape[1],)))\n",
        "#model.add(tf.keras.layers.Dense(64, activation=tf.nn.relu))\n",
        "#model.add(tf.keras.layers.Dense(5, activation=tf.nn.softmax))\n",
        "#model.compile(loss='categorical_crossentropy', optimizer=tf.train.RMSPropOptimizer(learning_rate=0.001), metrics=['accuracy'])\n",
        "\n",
        "#model.summary()\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Conv1D(filters=18, kernel_size=3, strides=1, padding='same', activation='relu', input_shape=twod_array[0].shape))\n",
        "#model.add(keras.layers.Conv2D(filters=64, kernel_size=(3,3), strides=1, padding='same', activation='relu')\n",
        "\n",
        "model.add(keras.layers.MaxPooling1D(pool_size= 2))\n",
        "model.add(keras.layers.Dropout(0.25))\n",
        "model.add(keras.layers.Flatten())\n",
        "\n",
        "# model.add(Dropout(0.5))\n",
        "model.add(keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "               optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(twod_array, train_labels, \n",
        "          batch_size=100,\n",
        "          epochs=20,\n",
        "          verbose=1)\n",
        "          #validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sOcj_uuUHP3Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### Run every time you change your model parameters ###\n",
        "\n",
        "#model = tf.keras.Sequential()\n",
        "\n",
        "\"\"\" Modify to your heart's and algorithm's content ^_^ \"\"\"\n",
        "\n",
        "#model.add(keras.layers.Conv1D(64, kernel_size=3,\n",
        "#                 activation=tf.nn.relu,\n",
        "#                 input_shape=(4000,7)))\n",
        "\n",
        "#model.add(tf.keras.layers.Dense(64, activation=tf.nn.relu, input_shape=(train_data.shape[1],)))\n",
        "#model.add(tf.keras.layers.Dense(64, activation=tf.nn.relu))\n",
        "#model.add(tf.keras.layers.Dense(5, activation=tf.nn.softmax))\n",
        "#model.compile(loss='categorical_crossentropy', optimizer=tf.train.RMSPropOptimizer(learning_rate=0.001), metrics=['accuracy'])\n",
        "\n",
        "#model.summary()\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Conv1D(filters=18, kernel_size=3, strides=1, padding='same', activation='relu', input_shape=twod_array[0].shape))\n",
        "#model.add(keras.layers.Conv2D(filters=64, kernel_size=(3,3), strides=1, padding='same', activation='relu')\n",
        "\n",
        "model.add(keras.layers.MaxPooling1D(pool_size= 2))\n",
        "model.add(keras.layers.Dropout(0.25))\n",
        "model.add(keras.layers.Flatten())\n",
        "\n",
        "# model.add(Dropout(0.5))\n",
        "model.add(keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "               optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(twod_array, train_labels, \n",
        "          batch_size=100,\n",
        "          epochs=20,\n",
        "          verbose=1)\n",
        "          #validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lhhsAXXJHRvJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### Run whenever you want to train and validate your model ###\n",
        "\n",
        "\"\"\" EPOCHS (int) is whatever number causes stabilization of validation loss and accuracy \"\"\"\n",
        "EPOCHS = 50\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    for signals, labels in training_set:\n",
        "        tr_loss, tr_accuracy = model.train_on_batch(signals, labels)\n",
        "    val_loss, val_accuracy = model.evaluate(val_data, val_labels)\n",
        "    print(('Epoch #%d\\t Training Loss: %.2f\\tTraining Accuracy: %.2f\\t'\n",
        "         'Validation Loss: %.2f\\tValidation Accuracy: %.2f')\n",
        "         % (epoch + 1, tr_loss, tr_accuracy,\n",
        "         val_loss, val_accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bduvOuTtHVfK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Submitting Your Model\n",
        "\n",
        "After training your classifier, run it on the test data to generate your predictions. Each class for a test sample should have an associated probability (between 0 and 1). Below are the parameters for the prediction format and export:\n",
        "\n",
        "- Your predictions should be in a pandas dataframe with 5 columns (classes) and 1000 rows (samples). Note that your predictions must follow the original test sample order (0.xz, 1.xz, 2.xz, ...). You only need to worry about this if you shuffled the test samples or stored the samples in an unordered data structure (dictionaries and sets). If this is the case, you should 1) add a separate column in your pandas dataframe with the file number for each sample; 2) sort the dataframe using this column; and 3) drop the column. These steps have been noted in the code below.\n",
        "- The predictions dataframe should be exported as an .xz file using dataframe.to_pickle() followed by files.download().\n",
        "\n",
        "Example code of the prediction format and export is presented in the cell block below. "
      ]
    },
    {
      "metadata": {
        "id": "CRk6Xh_LHTXB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### Run once after you have finished training your model ###\n",
        "\n",
        "test_pred = model.predict(test_data)\n",
        "test_output = np.ndarray(shape = (1000, 6))\n",
        "\n",
        "\"\"\" Add column with file number \"\"\"\n",
        "for i in range(1000):\n",
        "    test_output[i] = np.append(test_pred[i], test_order[i])\n",
        "test_dataframe = pd.DataFrame(test_output)\n",
        "\n",
        "\"\"\" Sort dataframe according to file number \"\"\"\n",
        "sorted_test_dataframe = test_dataframe.sort_values(by=[5])\n",
        "\n",
        "\"\"\" Drop file number column \"\"\"\n",
        "processed_test_dataframe = sorted_test_dataframe.drop(sorted_test_dataframe.columns[5], axis=1)\n",
        "\n",
        "print(test_dataframe.head(), '\\n\\n', sorted_test_dataframe.head(), '\\n\\n', processed_test_dataframe.head())\n",
        "\n",
        "file = 'PotatoesAreGreat.xz'\n",
        "processed_test_dataframe.to_pickle(file)\n",
        "files.download(file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8TiEUxHcZz_D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Your model will be evaluated on Area Under the ROC Curve (ROCAUC), Matthews Correlation Coefficient (MCC) and creativity. There will be a \"winning\" group for each of these categories.\n",
        "\n",
        "If you are finished early, consider trying other ML algorithms and/or implementing multiple feature extraction methods. You can also help other groups if you finish early."
      ]
    },
    {
      "metadata": {
        "id": "LDQWbhUK6cMt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## How Your Model Will Be Evaluated\n",
        "\n",
        "- **Area Under the ROC Curve (AUCROC)**: The receiver operating characteristic (ROC) curve plots the true positive rate (sensitivity/recall) against the false positive rate (fall-out) at many decision threshold settings. The area under the curve (AUC) measures discrimination, the classifier's ability to correctly identify samples from the \"positive\" and \"negative\" cases. Intuitively, AUC is the probability that a randomly chosen \"positive\" sample will be labeled as \"more positive\" than a randomly chosen \"negative\" sample. In the case of a multi-class ROC curve, each class is considered separately before taking the weighted average of all the class results. Simply put, the class under consideration is labeled as \"positive\" while all other classes are labeled as \"negative.\" Below is the multi-class ROC curve for the example classifier. The AUCROC score should be between 0 and 1, in which 0.5 is random classification and 1 is perfect classification.\n",
        "\n",
        " ![alt text](https://image.ibb.co/g4pzST/ROCAUC.png)\n",
        "\n",
        "- **Matthews Correlation Coefficient (MCC)**: The MCC measures the quality of binary classifications, irrespective of the class sizes. Importantly, it is typically regarded as a balanced measure since it considers all values in the 2x2 contingency table (TP, FP, TN, FN). For this challenge, the binary classes will be \"Arousal\" (Arousal) and \"Nonarousal\" (NREM1, NREM2, NREM3, REM). The MCC score should be between -1 and 1, in which 0 is random classification and 1 is perfect classification.\n",
        "\n",
        " ![alt text](https://wikimedia.org/api/rest_v1/media/math/render/svg/5caa90fc15105b74b59a30bbc9cc2e5bd43a13b7)\n",
        "\n",
        "Using these metrics, the example classifier has the following scores on test data:\n",
        "- AUCROC: 0.755\n",
        "- MCC: 0.353\n",
        "- Creativity: ( ͡° ͜ʖ ͡°)\n",
        "\n",
        "Below is the code used to calculate the AUCROC and MCC metrics when evaluating your classifier."
      ]
    },
    {
      "metadata": {
        "id": "vOpioHig8688",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_pred = pd.DataFrame(model.predict(mocktest_data))\n",
        "test_predict = test_pred.idxmax(axis=1)\n",
        "test_labels = [ np.where(label==1)[0][0] for label in mocktest_labels]\n",
        "test_labels_one_hot = pd.DataFrame(mocktest_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vIlfwfksLp-j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fpr = {}\n",
        "tpr = {}\n",
        "roc_auc = {}\n",
        "for i in range(5):\n",
        "    fpr[i], tpr[i], _ = metrics.roc_curve(test_labels_one_hot.iloc[:, i], test_pred.iloc[:, i])\n",
        "    roc_auc[i] = metrics.auc(fpr[i], tpr[i])\n",
        "    plt.plot(fpr[i], tpr[i], label = stage_dict[i] + ', ' + str(i))\n",
        "plt.plot([0, 1], [0, 1])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Multi-Class ROC Curve')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "fpr[\"micro\"], tpr[\"micro\"], _ = metrics.roc_curve(test_labels_one_hot.values.ravel(), test_pred.values.ravel())\n",
        "roc_auc = metrics.auc(fpr[\"micro\"], tpr[\"micro\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XO1I5lme8oya",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_true = []\n",
        "y_pred = []\n",
        "for i in range(test_pred.shape[0]):\n",
        "    if test_predict.iloc[i]==0: y_pred.append(1)\n",
        "    else: y_pred.append(-1)\n",
        "    if test_labels[i]==0: y_true.append(1)\n",
        "    else: y_true.append(-1)\n",
        "mcc = metrics.matthews_corrcoef(y_true, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fo7edcTTC6yD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(roc_auc, mcc)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}